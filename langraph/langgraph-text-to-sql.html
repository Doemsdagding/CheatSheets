<!DOCTYPE html>
<html>
  <head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <title>LANGGRAPH‚ÄìTEXT-TO-QUERY-AGENTS</title>
    <link rel="stylesheet" href="../stylesheet.css">
    <script src="../script.js"> </script>
    <style>
        /* Containers */
        .container1 {
            border: 6px solid #1C3C3C;
            border-radius: 7px;
            padding: 3px;

        }

        .container2 {
            border: 6px solid #00a532;
            border-radius: 7px;
            padding: 3px;

        }

        .container3 {
            border: 6px solid #cc5454;
            border-radius: 7px;
            padding: 3px;

        }

        .container4 {
            border: 6px solid #97e606;
            border-radius: 7px;
            padding: 3px;

        }
    </style>
  </head>
  <body class="light-mode">
    <!-- Back Button --> <button class="back-button"
      onclick="window.location.href='../'" title="Back to Home page"> √ó
    </button>
    <!--ICON-->
    <svg style="display: none;">
      <symbol id="myBase64Image" viewBox="0 0 1 1">
        <image
href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACMAAAAjCAYAAAAe2bNZAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAAAYdEVYdFNvZnR3YXJlAFBhaW50Lk5FVCA1LjEuOWxu2j4AAAC2ZVhJZklJKgAIAAAABQAaAQUAAQAAAEoAAAAbAQUAAQAAAFIAAAAoAQMAAQAAAAIAAAAxAQIAEAAAAFoAAABphwQAAQAAAGoAAAAAAAAAYAAAAAEAAABgAAAAAQAAAFBhaW50Lk5FVCA1LjEuOQADAACQBwAEAAAAMDIzMAGgAwABAAAAAQAAAAWgBAABAAAAlAAAAAAAAAACAAEAAgAEAAAAUjk4AAIABwAEAAAAMDEwMAAAAABMz8BIJY/XoAAAAupJREFUWEft1l+oZlMYx/HPeQ8yx7T9G2Jsu4ZT4+pc0KS0IheYDJIYfwohRG6MZG6MP8UM05By4U8ZEkrUoJM/TSIrMcPNuJAwadtlMm9z9JpE53DcrHfaZ3nnjE6duHi/N7v1rGet/axnPeu3FkOGDBmyuIzkhj5lCAVWYhlG8/4F8Ce6+KaJsZd3GhRMGcIEbsVFOAmHDfJbALOYwR68j+ebGHe1HQ78pAyhg7uxPmVjseliE55sYvxLO/1FVW3EwxhrDfgWL6TVnJ6ytFB+S9np/3MMF+LIXl1v1+8oQ7gTj84Zys+4sonxpV5dv15U1X5cgE7mdyimUgYewlspiJWtXQlFVe3t1fXOThnCONZlE8CWJsadrfYz+LTV/jdM4UY8jXNSIDfgzcxvXRnC+GhRVbfhqqwTpoqqGi+qqtur626vrmeKqtqNy7Akdx7AH7gmbfW2FNRafIWPcHVr24/DTx2sySbpsxYbcVPf0MT4MW7Bvrmu/2AP7sAnuCSdyj4r8Es6WW3WdDCRGXNWlSEcKOomxm0p1QO1ImXi0ibGrRjH97g8LWAHnsXFWJqNm+igyIw5Z+Q+TYyTuDcJWZvdacuLMoQHU4bOw6+4Hdel9j3ZOCg686ywz8lYnRuxFW+32rPYjN/xMh7AuXgXY02Mb+AsvIhjW+P69DqYo4IHYUMZwqq2oYlxGk8l/YD9+CwV5vJkexVn4+syhC14Jc9yi10dTObWAazA42UIR7eNqaA/T80lqUa2pwKF1/AEbk7yMZ9oTo4knXkvKezBmMGP6SSV+AF9oZzGOymQHbg++S/Hl6lWNh8ikO+werRX1/uKqpqZ54hP4zl8gSNwYlLuK3BtysgynIpTcH5S12NSNu7C4fmkGfc3MX7Qvig34b65PqTC3Ju+kmiVOAHHZ74L4bEmxvWyi/LDdATPzC7LERyVdGEpTkuZaPsshC424JFeXc8a9E75X7xncv6Ll96QIUOGLDZ/A4Nu1BUdA5uQAAAAAElFTkSuQmCC"
          width="1" height="1"></image> </symbol> </svg>
    <svg style="display: none;">
      <symbol id="langgraph" viewBox="0 0 1 1">
        <image
href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACMAAAAjCAYAAAAe2bNZAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAAAYdEVYdFNvZnR3YXJlAFBhaW50Lk5FVCA1LjEuN4vW9zkAAAC2ZVhJZklJKgAIAAAABQAaAQUAAQAAAEoAAAAbAQUAAQAAAFIAAAAoAQMAAQAAAAIAAAAxAQIAEAAAAFoAAABphwQAAQAAAGoAAAAAAAAAYAAAAAEAAABgAAAAAQAAAFBhaW50Lk5FVCA1LjEuNwADAACQBwAEAAAAMDIzMAGgAwABAAAAAQAAAAWgBAABAAAAlAAAAAAAAAACAAEAAgAEAAAAUjk4AAIABwAEAAAAMDEwMAAAAAAlR56NozS1xQAAAxlJREFUWEft1l+IVVUUx/HPvTMqVN4GLbI8HaLM6I/l2B8wdxmUUQRRUUQFRRREBBX40FM24JMPYQT9MQh86B+BUSCUBaXDLoosiSSZB0lPx6yQilskg6k9uO9w3I2jFT51vy+Xs9Zv3bP2Wnuvs+nTp0+f40srN0ARwjRcjAUYOpLuH/IHtmNzHeOvuVP+kiKEGbgDj+ACnIiBpuY/cBDj+A6v44U6xh+agolkihDm4Gnc3RQcR77Eo3WMH/cMAw4lMoTXcMthctZiJK3mEkzP/FPxGZbjJyxCO/Ofjus7Zflpt6pqGCiWLGlptVZNUpEPcU8d49ZuVX3QKcs/sSzTHIkDadXrOmW5CbfjlFyEDoY7ZbmuW1V721qtxbgvE+3Hs3WMexu2l7Gl8TwVbSwtQhjAYpyWCxoswgNS0F04ORMcwEgRwvoihGGoY/wZj+PHTDsZWzAP32I1PsoFTQbb7dsws41rcyemYSFuwg09Yx3jaKriYaegwQ6swBNpoWfiLKzD/XgrD4Adu78/+4uvt77Yxjm5M2NBEcJg76GOcUNKqHu4DKyuY1yJT7ABK7ET++oY16a4nXnQ0OzZp24cHd3YxsSLjsAwZjYNdYzv48mmLdFb2AxciMswC/OT/bz8v2Dm4DTLR1ZUrSKEXTgjF2Q8VMf4UtNQhNDBO7imYR7HNryCy3Fnsv+Or1Cm1uXsw7KBTllegYtyb8ZVnbLc1q2qsZ6hW1XjnbLcj1sbw3MQc3AlfsHbKZHzUyL5QekxhufbaXMdjSE8WIQw3aGqnFuEMDdtyIkJ2uAkvFnHuDztk+25IGNTHeOONt7F+tzbYC82pw3bm8D3YhTXYRX2ZDFwQvqddZTJvRvP6ZW3CGFhSmhurkxlfiOt9pvU+5txY5qgu1Jr8hbsSfNlfvqUTMZBPFzHuEb2oVyKNWnHT8V4miGtYziJU9HFU3WMz/QME9eDblXt7JTle+naMC8dz8kYTHH5h+9Y2Y9NeKyO8dWm42+XpiKEdrpYXY1LU8/bqaT/llZq9xg24vM6xt9yUZ8+ffr8r/gLgMPQLxhvs7YAAAAASUVORK5CYII="
          width="1" height="1"></image> </symbol> </svg>
    <svg style="display: none;">
      <symbol id="redshiftlgo" viewBox="0 0 1 1">
        <image
href="data:image/png;base64,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"
          width="1" height="1"></image> </symbol> </svg>
    <svg style="display: none;">
      <symbol id="pythonlogo" viewBox="0 0 1 1">
        <image
href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACIAAAAiCAYAAAA6RwvCAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAALEoAACxKAXd6dE0AAAAYdEVYdFNvZnR3YXJlAFBhaW50Lk5FVCA1LjEuN4vW9zkAAAC2ZVhJZklJKgAIAAAABQAaAQUAAQAAAEoAAAAbAQUAAQAAAFIAAAAoAQMAAQAAAAIAAAAxAQIAEAAAAFoAAABphwQAAQAAAGoAAAAAAAAACmUEAOgDAAAKZQQA6AMAAFBhaW50Lk5FVCA1LjEuNwADAACQBwAEAAAAMDIzMAGgAwABAAAAAQAAAAWgBAABAAAAlAAAAAAAAAACAAEAAgAEAAAAUjk4AAIABwAEAAAAMDEwMAAAAAAJW54+1ezNqwAACXhJREFUWEedl3+MXNV1xz/n3vdmZn+Md73Ga+9C4sbYBmMTh8o1JsRqZDfQ5gdNo4DIL4FalYiGmEQVrUJVojQSNK1SoHFbSNoipSRS0yRq1LQNNLUJJDQBg0rAxRiMf6693t87s7Mz7717z+kfszZ4vDg/PtLVjO68e+73fu85794RfgHed+8P0RgJIa8653/Toq6JoehRC0WMOh1jfDQWYX+SfZeG7uCZv76pM8QbIp0d5+Pdf7kb01j1SfmbGuPGkLeGYyxQjcQQUY3TRVFc77z77yJL2Xvf9Z0h3hDX2XE+RATn07eZWX99anR91PB4jIEQcoshJxTZUsx2Vi9Yhulc5/Dz8jMdOTo6w/YH93JptyNEBWEHxr8Ueesh03hTDPmyGILFEFAN0tRkz3P3vrR95oTQl9TAJ9A1juv5amfoszivI7997x4+/k/Psa47wcQj4ghFMaIh/AjYpBafVNUnVRXVKI1c2bx8fjjM5p/or+a/a/gBiwEbTztDn8N5hRSF4AWc2KDA3cDXvfN3RovjqnrUlFOqNqsxojEymkX7lf7GJVLEXTSzfxCKB93g51CD8R9t7Qx/FotujZ36A6w38OmvfZD7b7nYvfeLI9+JIX9vDAUxRjQWxBjQEIjtZjEWHJ5Xbts0L3ddfwgXckxCiJpfac36s/PT4zjv0CLQ//b/6ZzyXEd0dCda6cXme7hh0wi3PnDwKrP4WzEWtFce2p8hEmOwGIOpRlE1mcxFVg1kOGuCZkhoJdaY+nJWG7u2urQL56AVjPpT2/jfR9561rxnHAlHP410ATEg5Qqh1iilg3LzU/s23vmp/1yzqpecGAMxBkbnlWN5ZD4ECAWEAMF431DOAx87wHD/TLtfC7L5OiFvqnfyzzHGz5fL6YutVqC7kjJRn6fc26D/rc+3hcSxP0TziCt7XKWKFXPvwuSzIuHqx/at4bb/WsNKnxFCpJEHPrphistW1lE1TCMWAyVfsGHVJCuXzEARQHOwgqzZpNnM8E7wXmZF+JsQwl+VSslklgXSUkLWyhGd/iMsKq6Soo18vaTuT0T5EFEdLvDYi2u59ZHVDCUZIw3ljs3j/N72/YgDzEADaAGaQVxwSIu2EM1pNXNauSEL3jsneMcriN49MVF/eOnSauF8gkMBsxVWxL9w3v1YCvsIQR3WHthXyQgaiVEpNLLl4om2iMJhuUJeYHmB5QErCizmWMwgZmA5ZnZWSagaRbA1pu4fB5f3/cDU1ptGnDjDhH4RuRKzJdiCAgBzDC2bY3WlIC8iJQtU0gBRMc1AW0COyOmWIWQIObicuXrG2GRO6uWMIwCuXSInzfiOGifNwJF4xMlL8ZWXt5vjY5a6/Tih3RIa44dY0TxGA0/dBFVr2x4z0JyTJ6Y5/OokRw5NceTwNMePzzJyosbx4w3+9d9P8cDfHuQne6fIc8U5I0loponbZapbvJcvCMyIgNjeW7CLq1AokqZoqVR1qp+ojdc+8/3v7V/y2De/T+zayNTgtdSKlPtvq7HmTRlWtHPg5QNjNOpN0jQSi8Du3WMcOTiPSwQQfAK18TqXbFrGB2/ceOjCQfswPRf9uDV5jJ4r3kHtJ7vp2/oEyVzch73qICpJ6pHspFAa1FZDLG2NUu7toz79AkPFCyzPoBTeD7rkjCPr1vYQ84RDR2Z5y5pu9v1fmUMH5ymXHDFEmjPTbLj6cq657m0Mr1sx7Mv+DhP7857+lU/beIMl/ZejJ7fgLPGYGvPz8yVVbrby8FNB/RcqFde3/Z0Xctsn17PtNy4m125qdWg2W2AZxFa7UkKLyfE69929n7HJHFXDCTRnZ+iudnHjzvfz8dt3cOllQyRYmTx8QCKPW4i7LOqqOFxFJhtI7eltRLO1XSX/1SKyNcbXJSvgnWDAkaMNHnn0ONWBPj76kbV0VyLEfKFUI6OTBVPTOQ89eJC8lbPtuqt457su5YLBKkRrl/ppDPCClfwpi/phSfxuqe/dhkbrTkvuQ2ZyR1S7RPX1UtokXsgL5fl907z08hzr1lYZHExJEqNWj7xyuMGT33uV1Vdczrs/cAVr1y4HEYiLBGufpC+acI+G+G3xriH1Z7Zh0UjLnrwIA6XE364qt4egfe01vFZ3IuC9UKsVPPr4Ke646yDjC7+tXtbF3+26hl/f9hbKlRTCIgJEIJEZc3JfzMP9vpzOWJaD9+1Z6k9fTbnaRdbIKCWeVpZfmlT6vhKy+XeoRkReOxvNQMQhznFkpMU3vnuMvhWrufGGDVw03Nt24PXbwIIAM5TwOGn5FufkJQ0R66viJiZww196bbkH/mMLy5dXODQyx8Ytv0bSM/C5WJu5qzVxhJDNtsWIENVRRE+InqTUi5QHEAp6+gbwaUqaKE5OCxEQIWQNivoJpBj/s1Lfqs+6OIb1b0KyOnLhl08/eTb2DeA9d6JFvNsF+wxFIJubJJs9jsYmIJg5kp4hSkuGEPHMnXgOiFSWDNPVP4j49o1MQ05WO0VeP4F3Slry9ySp3Jmsf6xz2nPvI3LDwhc7sw+UlwzSO7SBcvUikspSqisvozr4ZsqVEqmPJInhJZDXDzM3uo98bpK8PsHc6D6y2lG8FCgJhSZkYfFr4zlCFsUUl5ToGngzvSsuIemqtkWaLZyN7S0QSdCiyfzEAZqTL2OhhXOeY9Or+LcXfpV6s5tXJwY7o8PPLYQFd04fyZ3J+HrELeSTw3tjZGaQh55dx1QzJXGKP5M/Z7O4EBEQEuS1uX9Zggq5QsmD85okyeIBFxViMSLGw5bIbpJz8vkXwsxx5XCD92x4ZU9/99TDb+ob7XwE3kiIpB6En+p47VoT+X1SdwT3ywjyrFo2efR3Nj15y0Vv/9Y1pu6n5sudD8EbCbGpFoSA6+sJ4uXvVe0qS92XSFx2bsGfiwgkXrIkkV0VX9+ahb6v5M9uDlZAbXLxv6KLCnHDX4SpHCsM+i9AhJNS8jvVdAep201y7rDT+Zt4IfGyB9Md5dR/stDyyb7NQzTylNGmsXLbDzuHwmIvtE5sz03ohgsgj9CVUMzM+tLA0ptF7U9RVmmeM3fieYSAT+Soc3x+ZKz50NBAJTaySCV1HDg5z+brnukMfRY/U8hpbGQn2lVq3+TKKVYUw+LTP7Yiu7V18gVBswdCsHvKZX+ilSlp6sgzZenWJzpDLcrPLeQ0OvYptNKPtOq4tCA2i23F9BEqlfSJ+vQU1UGYHXX0b/lB59Dz8v++Rj7WrStfVwAAAABJRU5ErkJggg=="
          width="1" height="1"></image> </symbol> </svg>
    <!--TOGGLE BUTTON--> <button class="toggle-button"
      onclick="toggleMode()" title="Toggle Dark/Light Mode"> <span
        class="icon"></span> </button>
    <!--PRINT BUTTON--> <button class="print-button" onclick="if
      (document.body.classList.contains('dark-mode')) toggleMode();
      window.print();" title="Print this page"> <span class="icon">üñ®Ô∏è</span>
    </button>
    <!-- Modal Structure -->
    <div id="modal" class="modal">
      <div class="modal-content"> <button class="modal-close"
          onclick="closeModal()" title="close modal">√ó</button>
        <!--next and back buttons--> <button class="modal-back"
          onclick="previousModal()" title="previous modal">&lt;</button>
        <button class="modal-next" onclick="nextModal()" title="next
          modal">&gt;</button> </div>
    </div>
    <!-- Swipe Hint -->
    <div id="swipe-hint" style="display: none; position: fixed; bottom:
      30px; left: 50%; transform: translateX(-50%); background-color:
      rgba(0, 0, 0, 0.7); color: white; padding: 20px 40px;
      border-radius: 10px; font-size: 30px; z-index: 2000; text-align:
      center; justify-content: center; align-items: center;"> Swipe left
      or right to navigate </div>
    <!--HEADER h1-->
    <h1>LANGGRAPH ‚Äì TEXT-TO-QUERY AGENTS
      <svg width="55" height="55">
        <use href="#redshiftlgo"></use> </svg> </h1>
    <!-- page 1 -->
    <table style="width: 765px; height: 998px;">
      <tbody>
        <tr>
          <td style="width: 765px;">
            <!-- single row -->
            <table width="765">
              <tbody>
                <tr>
                  <th>
                    <!--cell a1 -->
                    <div class="container1">
                      <table class="inner">
                        <tbody>
                          <tr>
                            <td>
                              <div style="display: flex;
                                justify-content: center; align-items:
                                center; position: relative;">
                                <div style="text-align: center;">
                                  <h2>Text to query agents<br>
                                  </h2>
                                </div>
                                <div style="position: absolute; right:
                                  -28px; top: -28px; z-index: 1;">
                                  <svg width="34" height="34">
                                    <use href="#myBase64Image"></use> </svg>
                                </div>
                                <div style="position: absolute; right:
                                  15px; top: -28px; z-index: 1;">
                                  <svg width="34" height="34">
                                    <use href="#myBase64Image"></use> </svg>
                                </div>
                              </div>
                            </td>
                          </tr>
                          <tr>
                            <td valign="top">AI agent that can query a
                              database to answer questions from your
                              data are often called text-to-query
                              agents. To achieve a text-to-query agent,
                              we need to combine two important concepts
                              in AI engineering: Retrieval Augmented
                              Generation, or RAG, and AI agents.<br>
                              the concepts and approraches of creating a
                              text to query agent is teh same fr any
                              database provider and querying language.<br>
                              <br>
                              how it works:<br>
                              1. the agent will take the prompt input
                              and forward this to the LLM, which is the
                              brain of the agent<br>
                              2.The LLM will likely realize that it
                              requires tools to respond, and begins by
                              identifying which tool to call The LLM
                              also identifies the arguments for the tool
                              call.<br>
                              3 Given a natural language user input, a
                              text-to-query application first needs to
                              identify which collection(tables for non
                              document storage) has the most relevant
                              information to answer the question, since
                              typically, your database will have
                              multiple collections.<br>
                              4 Then, it needs to determine the schema
                              of the documents in the identified
                              collection.<br>
                              5.The outcome of the tool execution is
                              returned to the LLM, <br>
                              5. Using this information, the application
                              can translate the natural language user
                              input into a valid MongoDB query.<br>
                              6. It's good practice to also validate the
                              generated query before executing it.<br>
                              7. And finally, the query is executed by a
                              seperate tool against the identified
                              collection to return a result to the llm<br>
                              <br>
                              to do all this the text-to-query agent
                              besides needing to be a chatbot that can
                              generate the code needs mulitple tools for
                              example for a MongoDb text-to-query agents
                              the needs:<br>
                              * DISCOVERY TOOL mongodb_list_collection
                              which list the tables present in the
                              database which helpts the agent identfy
                              which collection(table) to query and
                              obtain infromation for a particular user
                              query<br>
                              * SCHEMA UNDERSTANDING TOOL
                              mongodb_schema. given a list of
                              collections( tables) output the schema and
                              sample documents from the collection.
                              helps the agent identify what field names
                              to use when generating the mongodb queries<br>
                              * QUERY VALIDATION TOOL
                              mongodb_query_checker validates the
                              mongodb query generted by the llm<br>
                              * EXECUTION TOOL mongodb_query executes
                              the mongodb query<br>
                              <br>
                            </td>
                          </tr>
                          <tr>
                            <td valign="top">
                              <h3>Example (building toolbox using
                                premade tools for mongodb text-to-query
                                agent)</h3>
                            </td>
                          </tr>
                          <tr>
                            <td valign="top">import os<br>
                              from
                              langchain_mongodb.agent_toolkit.database
                              import MongoDBDatabase<br>
                              from
                              langchain_mongodb.agent_toolkit.toolkit
                              import MongoDBDatabaseToolkit<br>
                              from langchain_openai import ChatOpenAI<br>
                              import json<br>
                              <br>
                              <font color="#666666">#connect to mongodb
                                environment</font><br>
                              MONGODB_URI = os.environ["MONGODB_URI"]<br>
                              <br>
                              <font color="#666666"># Access the
                                sample_mflix database using the
                                MongoDBDatabase class</font><br>
                              db =
                              MongoDBDatabase.from_connection_string(connection_string=MONGODB_URI,
                              database="sample_mflix")<br>
                              <br>
                              llm = ChatOpenAI(<br>
                              &nbsp;&nbsp;&nbsp; model="gpt-4o-mini",<br>
                              &nbsp;&nbsp;&nbsp; temperature=0<br>
                              )<br>
                              <br>
                              <font color="#666666"># Initialize the
                                MongoDB database toolkit</font><br>
                              toolkit = MongoDBDatabaseToolkit(db=db,
                              llm=llm)<br>
                              <br>
                              <font color="#666666"># Extract the tools
                                from the toolkit</font><br>
                              tools = toolkit.get_tools()<br>
                              <br>
                              <font color="#666666"># Map tool names to
                                tool objects</font><br>
                              tool_map = {tool.name:tool for tool in
                              tools}<br>
                              <br>
                              <font color="#666666"># See what each tool
                                does</font><br>
                              {tool.name:tool.description for tool in
                              tools}<br>
                              <br>
                              <font color="#666666"># Test the
                                mongodb_list_collections tool</font><br>
tool_map["mongodb_list_collections"].invoke({})<br>
                              <br>
                              <font color="#666666"># Test the
                                mongodb_query_checker tool</font><br>
                              query = [{"$unwind": "$directors"},<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;

                              {"$group": {"_id": "$directors",
                              "filmCount": {"$sum": 1}, "avgRating":
                              {"$avg": "$imdb.rating"}}},<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;

                              {"$match": {"filmCount": {"$gte": 20}}},<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;

                              {"$sort": {"avgRating": -1}},<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;

                              {"$limit": 5}]<br>
                              <br>
tool_map["mongodb_query_checker"].invoke(json.dumps(query))<br>
                              <font color="#666666"><br>
                                # Test the mongodb_query tool</font><br>
                              print(tool_map["mongodb_query"].invoke(f"db.movies.aggregate({json.dumps(query)}"),

                              end="")<br>
                            </td>
                          </tr>
                          <tr>
                            <td valign="top">
                              <h3>Example (creating the text-to-query
                                agent using a premade imported prompt)</h3>
                            </td>
                          </tr>
                          <tr>
                            <td valign="top">from
                              langchain_mongodb.agent_toolkit import
                              MONGODB_AGENT_SYSTEM_PROMPT<br>
                              from langchain_core.prompts import
                              ChatPromptTemplate, MessagesPlaceholder<br>
                              <br>
                              <font color="#666666"># preview the pre
                                made agent prompt template</font><br>
                              MONGODB_AGENT_SYSTEM_PROMPT<br>
                              <br>
                              <font color="#666666"># Add aditional
                                instructions to the premade prompt
                                template</font><br>
                              prompt = ChatPromptTemplate.from_messages(<br>
                              &nbsp;&nbsp;&nbsp; [<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              ("system", MONGODB_AGENT_SYSTEM_PROMPT),<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              ("system", "Do not re-run tools unless
                              absolutely necessary. If you are not able
                              to get enough information using the tools,
                              reply with I DON'T KNOW. You have access
                              to the following tools: {tool_names}."),<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
MessagesPlaceholder(variable_name="messages"),<br>
                              &nbsp;&nbsp;&nbsp; ]<br>
                              )<br>
                              <br>
                              <font color="#666666"># Pre-fill top_k and
                                tool_names in the prompt template</font><br>
                              prompt = prompt.partial(top_k=5,
                              tool_names=", ".join([tool.name for tool
                              in tools]))<br>
                              <br>
                              <font color="#666666"># Bind tools to the
                                LLM</font><br>
                              tool_augmented_llm = llm.bind_tools(tools)<br>
                              <br>
                              <font color="#666666"># Chain the prompt
                                and tool-augmented LLM</font><br>
                              llm_with_tools = prompt |
                              tool_augmented_llm<br>
                              <br>
                              <font color="#666666"># Test the
                                llm_with_tools chain</font><br>
                              llm_with_tools.invoke({"messages":
                              [("user", "Give me the top 5 directors by
                              IMDB rating, who have made more than 20
                              movies")]}).tool_calls<br>
                            </td>
                          </tr>
                          <tr>
                            <td valign="top">
                              <h3>Example Orchestrating the
                                text-to-query agent</h3>
                            </td>
                          </tr>
                          <tr>
                            <td valign="top">import os<br>
                              from pymongo import MongoClient<br>
                              from typing import Annotated<br>
                              from langgraph.graph.message import
                              add_messages<br>
                              from typing_extensions import TypedDict<br>
                              from langchain_core.messages import
                              ToolMessage<br>
                              from langgraph.graph import StateGraph<br>
                              from langgraph.graph import START, END<br>
                              from langgraph.prebuilt import
                              tools_condition<br>
                              <br>
                              mongodb_client = MongoClient(MONGODB_URI)<br>
                              <br>
                              <font color="#666666">#Define the graph
                                state using the GraphState class, with a
                                single attribute called messages.</font><br>
                              class GraphState(TypedDict):<br>
                              &nbsp;&nbsp;&nbsp; messages:
                              Annotated[list, add_messages]<br>
                              <br>
                              <font color="#666666"># Define the agent
                                node</font><br>
                              def agent_node(state):<br>
                              &nbsp;&nbsp;&nbsp; <font color="#666666">#
                                Read "messages" from the graph state</font><br>
                              &nbsp;&nbsp;&nbsp; messages =
                              state["messages"]<br>
                              &nbsp;&nbsp;&nbsp; <font color="#666666">#
                                Invoke the tool-augmented LLM to
                                determine next action</font><br>
                              &nbsp;&nbsp;&nbsp; result =
                              llm_with_tools.invoke(messages)<br>
                              &nbsp;&nbsp;&nbsp; <font color="#666666">#
                                Append the result to the graph state</font><br>
                              &nbsp;&nbsp;&nbsp; return {"messages":
                              [result]}<br>
                              <br>
                              <font color="#666666"># Define the tool
                                node</font><br>
                              def tool_node(state: GraphState):<br>
                              &nbsp;&nbsp;&nbsp; result = []<br>
                              &nbsp;&nbsp;&nbsp; <font color="#666666">#
                                Read the tool invocation payload from
                                the graph state</font><br>
                              &nbsp;&nbsp;&nbsp; tool_calls =
                              state["messages"][-1].tool_calls<br>
                              &nbsp;&nbsp;&nbsp; for tool_call in
                              tool_calls:<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              <font color="#666666"># Extract the name
                                of the tool to execute</font><br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              tool = tool_map[tool_call["name"]]<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              <font color="#666666"># Invoke the tool
                                with the arguments extracted from the
                                payload</font><br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              observation =
                              tool.invoke(tool_call["args"])<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              <font color="#666666"># Create a
                                ToolMessage to log the result of the
                                tool execution</font><br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              result.append(ToolMessage(content=observation,
                              tool_call_id=tool_call["id"]))<br>
                              &nbsp;&nbsp;&nbsp; <font color="#666666">#
                                Append the result to the graph state</font><br>
                              &nbsp;&nbsp;&nbsp; return {"messages":
                              result}<br>
                              <br>
                              <font color="#666666">#Build the graph<br>
                                # Initialize the graph with graph state</font><br>
                              graph = StateGraph(GraphState)<br>
                              <br>
                              <font color="#666666"># Add a node called
                                "agent" that calls the agent_node()
                                function</font><br>
                              graph.add_node("agent", agent_node)<br>
                              <font color="#666666"># Add a node called
                                "tools" that calls the tool_node()
                                function</font><br>
                              graph.add_node("tools", tool_node)<br>
                              <br>
                              <font color="#666666"># Add a fixed edge
                                from the START node to the "agent" node</font><br>
                              graph.add_edge(START, "agent")<br>
                              <font color="#666666"># Add a fixed edge
                                from the "tool" node to the "agent" node</font><br>
                              graph.add_edge("tools", "agent")<br>
                              <font color="#666666"># Add conditional
                                edges from the "agent" node to the
                                "tool" and "END" nodes</font><br>
                              graph.add_conditional_edges(<br>
                              &nbsp;&nbsp;&nbsp; "agent",<br>
                              &nbsp;&nbsp;&nbsp; tools_condition,<br>
                              &nbsp;&nbsp;&nbsp; {"tools": "tools", END:
                              END},<br>
                              )<br>
                              <br>
                              <font color="#666666"># Compile and
                                visualize the graph</font><br>
                              app = graph.compile()<br>
                              app<br>
                              <font color="#666666"><br>
                                #Test the agent</font><br>
                              def execute_graph(user_input: str) -&gt;
                              None:<br>
                              &nbsp;&nbsp;&nbsp; """<br>
                              &nbsp;&nbsp;&nbsp; Stream outputs from the
                              graph<br>
                              <br>
                              &nbsp;&nbsp;&nbsp; Args:<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              user_input (str): User query string<br>
                              &nbsp;&nbsp;&nbsp; """<br>
                              &nbsp;&nbsp;&nbsp; <font color="#666666">#
                                Stream outputs from each step in the
                                graph</font><br>
                              &nbsp;&nbsp;&nbsp; for step in app.stream(<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              {"messages": [{"role": "user", "content":
                              user_input}]},<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              <font color="#666666"># Stream full value
                                of the state after each step</font><br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              stream_mode="values",<br>
                              &nbsp;&nbsp;&nbsp; ):<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font
                                color="#666666"> # Print the latest
                                message from the step</font><br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              step["messages"][-1].pretty_print()<br>
                              <br>
                              <font color="#666666">#test prompt</font><br>
                              execute_graph("Give me the top 5 directors
                              by IMDB rating, who have made at least 20
                              movies")<br>
                            </td>
                          </tr>
                        </tbody>
                      </table>
                    </div>
                  </th>
                </tr>
                <tr>
                  <td>
                    <!--cell a2 -->
                    <div class="container2">
                      <table class="inner">
                        <tbody>
                          <tr>
                            <td>
                              <div style="display: flex;
                                justify-content: center; align-items:
                                center; position: relative;">
                                <div style="text-align: center;">
                                  <h2>Adding memory 1: Short-term memory<br>
                                  </h2>
                                </div>
                                <div style="position: absolute; right:
                                  -28px; top: -28px; z-index: 1;">
                                  <svg width="34" height="34">
                                    <use href="#myBase64Image"></use> </svg>
                                </div>
                                <div style="position: absolute; right:
                                  15px; top: -28px; z-index: 1;">
                                  <svg width="34" height="34">
                                    <use href="#myBase64Image"></use> </svg>
                                </div>
                              </div>
                            </td>
                          </tr>
                          <tr>
                            <td valign="top">short term memory holds
                              context during the conversation only
                              lasting a single session.<br>
                              <br>
                              memory in langchain consistes of
                              checkpoints on threads with a thread being
                              a conversion and checkpoints being save
                              states aka agent states that can be made
                              at any given time durring the thread,
                              these are typically stored in-memory (as
                              short term memory).<br>
                              <br>
                              these checkpoints can also be saved so the
                              track can be resumed another time in a
                              second session (this does not fall under
                              long-term memory as it doesnt extract
                              specific insights from conversation for
                              long-term use) to setup this short-term
                              memory MongDBSaver can be used for storing
                              checkpoints across runs when using
                              MongoDB. checkpoints can be used in
                              various ways you can use them to analyuze
                              and debug agents, reset the agent to a
                              previous state (checkpoint) or even modify
                              information in acheckpoint afterwards to
                              imrpove follow up questions.<br>
                              <br>
                              short-term memory helps agents have
                              effective multi-turn conversations with
                              users and prevents agents from repeating
                              actions ( as it can look through its own
                              answers) and losing context between
                              interactions<br>
                            </td>
                          </tr>
                          <tr>
                            <td valign="top">
                              <h3>Example</h3>
                            </td>
                          </tr>
                          <tr>
                            <td valign="top">from
                              langgraph.checkpoint.mongodb import
                              MongoDBSaver<br>
                              from uuid import uuid4<br>
                              import pprint<br>
                              <font color="#666666">#setup all tools and
                                nodes and setup the edges ect.</font><br>
                              ...<br>
                              <br>
                              <font color="#666666">#setup checkpointer</font><br>
                              checkpointer =
                              MongoDBSaver(mongodb_client)<br>
                              <br>
                              app =
                              graph.compile(checkpointer=checkpointer)<br>
                              <br>
                              <font color="#666666"><br>
                                # set up the agent function (now
                                including thread_id) </font><br>
                              def execute_graph_with_memory(thread_id:
                              str, user_input: str) -&gt; None:<br>
                              &nbsp; """<br>
                              &nbsp;&nbsp;&nbsp; Execute the
                              memory-augmented agent<br>
                              <br>
                              &nbsp;&nbsp;&nbsp; Args:<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              thread_id (str): Thread ID for which to
                              retrieve memory<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              user_input (str): User query<br>
                              &nbsp;&nbsp;&nbsp; """<br>
                              <font color="#666666">&nbsp; # Configure
                                the thread ID</font><br>
                              &nbsp; config = {"configurable":
                              {"thread_id": thread_id}}<br>
                              <font color="#666666">&nbsp; # Stream
                                outputs from each step in the graph</font><br>
                              &nbsp; for step in app.stream(<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              {"messages": [{"role": "user", "content":
                              user_input}]},<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; config,<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              stream_mode="values",<br>
                              &nbsp; ):<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font
                                color="#666666"># Print the latest
                                message from the step</font><br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              step["messages"][-1].pretty_print()<br>
                              <br>
                              <font color="#666666">#execute test</font><br>
                              thread_id = str(uuid4())<br>
                              execute_graph_with_memory(thread_id,
                              "Which states have the most theaters?")<br>
                              execute_graph_with_memory(thread_id, "How
                              many theaters does California have?")<br>
                            </td>
                          </tr>
                          <tr>
                            <td valign="top">
                              <h3>Example (retrieving the checkpoints)</h3>
                            </td>
                          </tr>
                          <tr>
                            <td valign="top">config = {"configurable":
                              {"thread_id": thread_id}}<br>
                              <br>
                              <font color="#666666"># Reset agent to a
                                specific checkpoint</font><br>
                              checkpointer.get(config)<br>
                              app =
                              graph.compile(checkpointer=checkpointer)<br>
                              <br>
                              execute_graph_with_memory(thread_id, "How
                              many theaters does California have?")<br>
                              <font color="#666666"><br>
                                # List and analyze checkpoints</font><br>
                              pprint.pprint(list(checkpointer.list(config,
                              limit=5)))<br>
                            </td>
                          </tr>
                        </tbody>
                      </table>
                    </div>
                  </td>
                </tr>
                <tr>
                  <td>
                    <!--cell a3 -->
                    <div class="container1">
                      <table class="inner">
                        <tbody>
                          <tr>
                            <td>
                              <h2>Adding memory 2: long-term memory</h2>
                            </td>
                          </tr>
                          <tr>
                            <td valign="top">Long-term memory stores
                              knowledge over multiple sessions which can
                              be recalled. This is done through saving
                              checkpoints and using them to extract
                              insights that can be used in later
                              conversations.<br>
                              <br>
                              To set this up, the extracted knowledge is
                              saved into a persistent and searchable
                              database. In the context of using MongoDB,
                              this is achieved by storing the vectorized
                              knowledge in what is often referred to
                              conceptually as a MongoDBStore.
                              Specifically, key facts, entities, and
                              summaries are extracted from the
                              conversation history, converted into
                              vector embeddings, and stored in a
                              searchable knowledge base (typically
                              implemented using MongoDB Atlas Vector
                              Search).<br>
                              <br>
                              This extracted knowledge is then retrieved
                              (RAG) at the start of any new session and
                              injected into the agent's prompt, allowing
                              the agent to maintain consistency,
                              leverage a shared knowledge base, and
                              recall relevant details across completely
                              separate threads<br>
                            </td>
                          </tr>
                          <tr>
                            <td valign="top"><br>
                            </td>
                          </tr>
                          <tr>
                            <td valign="top">import json<br>
                              from uuid import uuid4<br>
                              from typing import TypedDict, Annotated,
                              List<br>
                              from langgraph.graph import StateGraph,
                              END<br>
                              from langgraph.checkpoint.mongodb import
                              MongoDBSaver<br>
                              <br>
                              <font color="#666666">#setup all tools and
                                nodes and setup the edges ect.</font><br>
                              ...<br>
                              <br>
                              <br>
                              <font color="#666666"># --- 1. Conceptual
                                MongoDB Vector Store for Long-Term
                                Memory ---</font><br>
                              <br>
                              class ConceptualMongoDBVectorStore:<br>
                              &nbsp;&nbsp;&nbsp; """A conceptual class
                              simulating a MongoDB Vector Store (RAG
                              Knowledge Base)."""<br>
                              &nbsp;&nbsp;&nbsp; def __init__(self):<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              <font color="#666666"># In a real app,
                                this would connect to MongoDB Atlas
                                Vector Search</font><br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              print("Initialised
                              ConceptualMongoDBVectorStore for RAG.")<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              self.db = []&nbsp; # In-memory storage for
                              demonstration<br>
                              <br>
                              &nbsp;&nbsp;&nbsp; def add_documents(self,
                              documents: List[str], metadata:
                              List[dict]):<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              """Simulates storing facts/summaries as
                              searchable vectors."""<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              for doc, meta in zip(documents, metadata):<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              self.db.append({"content": doc,
                              "metadata": meta})<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              print(f"--- Extracted and stored
                              {len(documents)} facts into Long-Term KB.
                              ---")<br>
                              <br>
                              &nbsp;&nbsp;&nbsp; def retrieve(self,
                              query: str) -&gt; List[str]:<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              """Simulates querying the Vector Store for
                              relevant context."""<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              print(f"--- Retrieving Long-Term Context
                              for query: '{query[:50]}...' ---")<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              <font color="#666666"># Simple keyword
                                matching for demo; a real Vector Store
                                uses embeddings</font><br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              results = [d['content'] for d in self.db
                              if query.lower() in
                              d['content'].lower()][:2]<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              if results:<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              print(f"--- Found {len(results)} relevant
                              long-term facts. ---")<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              return [f"Fact from past conversation:
                              {r}" for r in results]<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              return ["Fact from past conversation: No
                              relevant long-term context found."]<br>
                              <br>
                              # --- 2. Define Agent State and Nodes ---<br>
                              <br>
                              <font color="#666666"># Define the state
                                for the LangGraph</font><br>
                              class AgentState(TypedDict):<br>
                              &nbsp;&nbsp;&nbsp; messages:
                              Annotated[List[dict], lambda x, y: x + y]<br>
                              &nbsp;&nbsp;&nbsp; long_term_context: str
                              # New field for RAG-retrieved context<br>
                              <font color="#666666"><br>
                                # Initialize the conceptual Long-Term
                                Knowledge Base</font><br>
                              long_term_knowledge_base =
                              ConceptualMongoDBVectorStore()<br>
                              <br>
                              <br>
                              def retrieve_long_term_context(state:
                              AgentState) -&gt; AgentState:<br>
                              &nbsp;&nbsp;&nbsp; """<br>
                              &nbsp;&nbsp;&nbsp; Node to query the
                              Long-Term KB (Vector Store) based on the
                              current user input.<br>
                              &nbsp;&nbsp;&nbsp; Runs at the start of
                              the graph.<br>
                              &nbsp;&nbsp;&nbsp; """<br>
                              &nbsp;&nbsp;&nbsp; latest_user_message =
                              state['messages'][-1]['content']<br>
                              &nbsp;&nbsp;&nbsp; <br>
                              &nbsp;&nbsp;&nbsp; <font color="#666666">#
                                Retrieve context from the conceptual RAG
                                system</font><br>
                              &nbsp;&nbsp;&nbsp; retrieved_facts =
                              long_term_knowledge_base.retrieve(latest_user_message)<br>
                              &nbsp;&nbsp;&nbsp; <font color="#666666"><br>
                                &nbsp;&nbsp;&nbsp; # Join the facts into
                                a string to inject into the LLM prompt</font><br>
                              &nbsp;&nbsp;&nbsp; context_string =
                              "\n".join(retrieved_facts)<br>
                              &nbsp;&nbsp;&nbsp; <br>
                              &nbsp;&nbsp;&nbsp; return
                              {"long_term_context": context_string}<br>
                              <br>
                              def llm_node(state: AgentState) -&gt;
                              AgentState:<br>
                              &nbsp;&nbsp;&nbsp; """<br>
                              &nbsp;&nbsp;&nbsp; Node simulating the LLM
                              that uses both short-term history (from
                              'messages') <br>
                              &nbsp;&nbsp;&nbsp; and long-term RAG
                              context ('long_term_context').<br>
                              &nbsp;&nbsp;&nbsp; """<br>
                              &nbsp;&nbsp;&nbsp; user_query =
                              state['messages'][-1]['content']<br>
                              &nbsp;&nbsp;&nbsp; context =
                              state['long_term_context']<br>
                              &nbsp;&nbsp;&nbsp; <br>
                              &nbsp;&nbsp;&nbsp;<font color="#666666"> #
                                The LLM's full prompt incorporates the
                                retrieved long-term context</font><br>
                              &nbsp;&nbsp;&nbsp; llm_response = (<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              f"Retrieved Long-Term Context:
                              {context}\n\n"<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              f"Agent Response to '{user_query}': "<br>
                              &nbsp;&nbsp;&nbsp; )<br>
                              &nbsp;&nbsp;&nbsp; <br>
                              &nbsp;&nbsp;&nbsp; <font color="#666666">#
                                Simple simulated response based on the
                                presence of context</font><br>
                              &nbsp;&nbsp;&nbsp; if "California" in
                              user_query and "California has 500" in
                              context:<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              llm_response += "Based on my long-term
                              memory from a previous discussion,
                              California has 500 theaters."<br>
                              &nbsp;&nbsp;&nbsp; elif "the most
                              theaters" in user_query:<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              llm_response += "I'll first search for the
                              latest data on which states have the most
                              theaters."<br>
                              &nbsp;&nbsp;&nbsp; else:<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              llm_response += "Processing the query
                              using the available short-term and
                              long-term context."<br>
                              <br>
                              &nbsp;&nbsp;&nbsp; <font color="#666666">#
                                Update state with the new message</font><br>
                              &nbsp;&nbsp;&nbsp; new_message = {"role":
                              "assistant", "content": llm_response}<br>
                              &nbsp;&nbsp;&nbsp; return {"messages":
                              [new_message]}<br>
                              <br>
                              def fact_extraction_node(state:
                              AgentState) -&gt; AgentState:<br>
                              &nbsp;&nbsp;&nbsp; """<br>
                              &nbsp;&nbsp;&nbsp; Node that runs at the
                              end of a successful thread to extract and
                              store facts <br>
                              &nbsp;&nbsp;&nbsp; into the Long-Term KB.<br>
                              &nbsp;&nbsp;&nbsp; """<br>
                              &nbsp;&nbsp;&nbsp; <font color="#666666">#
                                Simulate extracting a key fact from the
                                conversation history<br>
                                &nbsp;&nbsp;&nbsp; # In a real app, this
                                would use a separate LLM call to
                                summarize the thread</font><br>
                              &nbsp;&nbsp;&nbsp; <font color="#666666"><br>
                                &nbsp;&nbsp;&nbsp; # Check the last
                                message content to decide what 'fact' to
                                save</font><br>
                              &nbsp;&nbsp;&nbsp; last_message =
                              state['messages'][-1]['content']<br>
                              &nbsp;&nbsp;&nbsp; if "500 theaters" in
                              last_message:<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              fact = "California has 500 theaters, as
                              discussed on this thread."<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              metadata = {"thread_id":
                              "completed_thread", "topic": "theaters"}<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              long_term_knowledge_base.add_documents([fact],
                              [metadata])<br>
                              &nbsp;&nbsp;&nbsp; <br>
                              &nbsp;&nbsp;&nbsp; <font color="#666666">#
                                Return the state unchanged for the next
                                step</font><br>
                              &nbsp;&nbsp;&nbsp; return state<br>
                              <br>
                              <br>
                              <font color="#666666"># --- 3. Build and
                                Compile the Graph ---<br>
                                <br>
                                # This represents your conceptual
                                MongoDB client connection</font><br>
                              mongodb_client = None<br>
                              <br>
                              <font color="#666666"># Setup checkpointer
                                (Short-Term Memory)</font><br>
                              checkpointer =
                              MongoDBSaver(mongodb_client)<br>
                              <br>
                              workflow = StateGraph(AgentState)<br>
                              <br>
                              <font color="#666666"># 1. Start by
                                retrieving long-term context</font><br>
                              workflow.add_node("retrieve_context",
                              retrieve_long_term_context)<br>
                              <font color="#666666"># 2. Run the main
                                LLM agent</font><br>
                              workflow.add_node("llm", llm_node)<br>
                              <font color="#666666"># 3. Extract facts
                                for long-term storage</font><br>
                              workflow.add_node("fact_extractor",
                              fact_extraction_node)<br>
                              <br>
                              <font color="#666666"># Define edges</font><br>
workflow.set_entry_point("retrieve_context")<br>
                              workflow.add_edge("retrieve_context",
                              "llm")<br>
                              workflow.add_edge("llm", "fact_extractor")<br>
                              workflow.add_edge("fact_extractor", END)<br>
                              <br>
                              app_long_term =
                              workflow.compile(checkpointer=checkpointer)<br>
                              <br>
                              <font color="#666666"># --- 4. Execution
                                Example ---</font><br>
                              <br>
                              def execute_long_term_agent(thread_id:
                              str, user_input: str):<br>
                              &nbsp;&nbsp;&nbsp; """Execute the
                              long-term memory agent and print
                              output."""<br>
                              &nbsp;&nbsp;&nbsp;
                              print(f"\n====================== THREAD:
                              {thread_id} ======================")<br>
                              &nbsp;&nbsp;&nbsp; <br>
                              <font color="#666666">&nbsp;&nbsp;&nbsp; #
                                Configure the thread ID for short-term
                                memory (checkpointer)</font><br>
                              &nbsp;&nbsp;&nbsp; config =
                              {"configurable": {"thread_id": thread_id}}<br>
                              &nbsp;&nbsp;&nbsp; <br>
                              &nbsp;&nbsp;&nbsp;<font color="#666666"> #
                                Stream outputs</font><br>
                              &nbsp;&nbsp;&nbsp; for step in
                              app_long_term.stream(<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              {"messages": [{"role": "user", "content":
                              user_input}]},<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              config,<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              stream_mode="values",<br>
                              &nbsp;&nbsp;&nbsp; ):<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font
                                color="#666666"> # Print the latest
                                message content and the retrieved
                                context</font><br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              print(f"Long-Term Context used:
                              {step.get('long_term_context', 'N/A')}")<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              if step['messages'][-1]['role'] ==
                              'assistant':<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              print(f"Agent Output:
                              {step['messages'][-1]['content']}")<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                              print("-" * 20)<br>
                              <br>
                              <br>
                              <font color="#666666"># --- TEST 1:
                                Initial conversation and fact extraction
                                (Saving Long-Term Memory) ---</font><br>
                              thread_id_1 = str(uuid4())<br>
                              <br>
                              print("--- PHASE 1: Conversation 1 (Thread
                              1: Saving a new fact) ---")<br>
                              execute_long_term_agent(thread_id_1, "I
                              was recently looking at California theater
                              data, and they have 500.")<br>
                              <font color="#666666"># The
                                fact_extraction_node will detect the
                                '500' and save it to the conceptual KB.</font><br>
                              <br>
                              <br>
                              <font color="#666666"># --- TEST 2: New
                                conversation thread (Retrieving
                                Long-Term Memory) ---</font><br>
                              thread_id_2 = str(uuid4())<br>
                              <br>
                              print("\n\n--- PHASE 2: Conversation 2
                              (Thread 2: Retrieving the saved fact)
                              ---")<br>
                              <font color="#666666"># This thread knows
                                nothing about thread_id_1, but the RAG
                                node will retrieve the fact<br>
                                execute_long_term_agent(thread_id_2,
                                "How many theaters does California
                                have?")<br>
                                <br>
                                # --- List and analyze checkpoints
                                (Still the short-term function) ---</font><br>
                              print("\n\n--- LISTING SHORT-TERM
                              CHECKPOINTS FOR THREAD 2 ---")<br>
                              config_t2 = {"configurable": {"thread_id":
                              thread_id_2}}<br>
                              <font color="#666666"># This shows the
                                session state of Thread 2 only</font><br>
                              print("Checkpoints for Thread 2
                              (Short-Term Memory):")<br>
                              <font color="#666666"># NOTE: Cannot run
                                checkpointer.list() without a real
                                MongoDB client, so we will<br>
                                # only print the expected structure
                                based on the conceptual KB updates.<br>
                                #
                                pprint.pprint(list(checkpointer.list(config_t2,
                                limit=5)))</font><br>
                              <br>
                              print("\n--- Listing Conceptual Long-Term
                              Facts Stored ---")<br>
                              print(json.dumps(long_term_knowledge_base.db,
                              indent=2))<br>
                            </td>
                          </tr>
                        </tbody>
                      </table>
                    </div>
                  </td>
                </tr>
              </tbody>
            </table>
          </td>
        </tr>
      </tbody>
    </table>
    <footer style="text-align: center; margin-top: 20px;">
      <p style="text-align: center;"> This website is a collection of my
        personal study notes, created to help me and others learn and
        organize information effectively. Feel free to <strong>download</strong>,
        <strong>print</strong>, or <strong>use</strong> any of the
        notes for your own purposes. </p>
      <p style="text-align: center;"> <br>
        Want to keep your own notes in the same format? Check out the <a
          href="Template/HTMLTEMPLATE" style="color: #1E90FF;
          text-decoration: underline;">Notes Template</a>! </p>
      <p style="text-align: center;"> Interested in forking this
        website? Visit my <a
          href="https://github.com/Doemsdagding/CheatSheets"
          target="_blank" style="color: #1E90FF; text-decoration:
          underline;">GitHub Repository</a> to get the full source code.
      </p>
      <p style="text-align: center;"> Happy learning and dont forget to
        share, follow and star the repository! -DoemsDagDing </p>
    </footer>
  </body>
</html>
